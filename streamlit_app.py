import streamlit as st
import pandas as pd
import os
from utils import get_reddit_hot, get_ai_news, get_github_trending, get_xhs_trends
from db_utils import supabase
from datetime import datetime, date

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="AI & IndieDev Daily",
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded"
)

# éšè— Streamlit é»˜è®¤çš„èœå•å’Œé¡µè„š
hide_streamlit_style = """
            <style>
            #MainMenu {visibility: hidden;}
            footer {visibility: hidden;}
            /* éšè— Deploy æŒ‰é’® */
            .stDeployButton {display:none;}
            </style>
            """
st.markdown(hide_streamlit_style, unsafe_allow_html=True)

# ä¾§è¾¹æ 
with st.sidebar:
    st.title("ğŸ“… æ—¥æœŸé€‰æ‹©")
    selected_date = st.date_input(
        "é€‰æ‹©è¦æŸ¥çœ‹çš„æ—¥æœŸ",
        value=date.today(),
        max_value=date.today()
    )
    
    st.divider()
    st.title("å…³äº")
    st.info(
        """
        è¿™ä¸ª Dashboard èšåˆäº†ï¼š
        1. æ¯æ—¥ AI æœ€æ–°åŠ¨æ€ (RSS)
        2. Reddit ç‹¬ç«‹å¼€å‘çƒ­é—¨éœ€æ±‚
        3. GitHub å½“æ—¥çƒ­æ¦œ
        4. å°çº¢ä¹¦çƒ­ç‚¹ (ç¾å¦†/æ‹ç…§éœ€æ±‚)
        
        æ•°æ®æºï¼š
        - OpenAI Blog, TechCrunch AI, etc.
        - r/indiehackers, r/SaaS, etc.
        - GitHub Trending
        - Bing Search (site:xiaohongshu.com)
        """
    )
    
    # åˆ·æ–°æŒ‰é’® (åªåœ¨ä»Šå¤©æœ‰æ•ˆï¼Œæˆ–è€…å¼ºåˆ¶åˆ·æ–°)
    if st.button("ğŸ”„ åˆ·æ–°æ•°æ®"):
        st.cache_data.clear()
        st.rerun()

    st.divider()
    # Supabase çŠ¶æ€æŒ‡ç¤ºå™¨
    if supabase:
        st.success("âœ… Supabase æ•°æ®åº“å·²è¿æ¥")
    else:
        st.error("âŒ Supabase æœªè¿æ¥ (æ•°æ®æ— æ³•ä¿å­˜)")
        st.caption("è¯·æ£€æŸ¥ Streamlit Secrets é…ç½®ä¸­æ˜¯å¦åŒ…å« SUPABASE_URL å’Œ SUPABASE_KEY")

# æ ‡é¢˜
st.title(f"ğŸš€ AI & IndieDev Daily ({selected_date.strftime('%Y-%m-%d')})")

# åŠ è½½æ•°æ®å‡½æ•°
@st.cache_data(ttl=3600)
def load_data(target_date):
    ai_news = get_ai_news(target_date)
    reddit_hot = get_reddit_hot(target_date)
    github_trending = get_github_trending(target_date)
    xhs_trends = get_xhs_trends(target_date)
    return ai_news, reddit_hot, github_trending, xhs_trends

# åŠ è½½æ•°æ®
with st.spinner('æ­£åœ¨è·å–æœ€æ–°æ•°æ®...'):
    ai_data, reddit_data, github_data, xhs_data = load_data(selected_date)

# æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
if ai_data.empty and reddit_data.empty and github_data.empty and xhs_data.empty:
    st.warning(f"æ²¡æœ‰æ‰¾åˆ° {selected_date} çš„å½’æ¡£æ•°æ®ã€‚å¦‚æœæ˜¯ä»Šå¤©ï¼Œå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼›å¦‚æœæ˜¯å†å²æ—¥æœŸï¼Œè¯´æ˜å½“æ—¶æ²¡æœ‰æŠ“å–ã€‚")
else:
    # é¡µé¢å¸ƒå±€
    tab1, tab2, tab3, tab4 = st.tabs(["ğŸ¤– æ¯æ—¥ AI åŠ¨æ€", "ğŸ”¥ ç‹¬ç«‹å¼€å‘çƒ­é—¨", "ğŸ“ˆ GitHub çƒ­æ¦œ", "ğŸ“• å°çº¢ä¹¦çƒ­ç‚¹"])

    with tab1:
        st.header("æ¯æ—¥ AI æœ€æ–°åŠ¨æ€")
        if not ai_data.empty:
            for index, row in ai_data.iterrows():
                # ä½¿ç”¨ row['published_str'] æ›¿ä»£ row['published']
                pub_time = row.get('published_str', str(row['published']))
                with st.expander(f"**{row['title']}** - *{row['source']}*"):
                    st.write(f"**å‘å¸ƒæ—¶é—´:** {pub_time}")
                    st.write(row['summary'])
                    st.markdown(f"[é˜…è¯»å…¨æ–‡]({row['link']})")
        else:
            st.info("æš‚æ—  AI åŠ¨æ€æ•°æ®")

    with tab2:
        st.header("Reddit ç‹¬ç«‹å¼€å‘çƒ­é—¨è®¨è®º")
        if not reddit_data.empty:
            display_df = reddit_data.copy()
            display_df['score'] = display_df['score'].apply(lambda x: "-" if pd.isna(x) or x == "N/A" else x)
            display_df['comments'] = display_df['comments'].apply(lambda x: "-" if pd.isna(x) or x == "N/A" else x)
            st.dataframe(
                display_df[['title', 'score', 'comments', 'source', 'created_utc', 'url']],
                column_config={
                    "url": st.column_config.LinkColumn("é“¾æ¥"),
                    "title": "æ ‡é¢˜",
                    "score": "çƒ­åº¦",
                    "comments": "è¯„è®ºæ•°",
                    "source": "æ¿å—",
                    "created_utc": "å‘å¸ƒæ—¶é—´"
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.info("æš‚æ—  Reddit æ•°æ®")

    with tab3:
        st.header("GitHub å½“æ—¥çƒ­æ¦œ")
        if not github_data.empty:
            st.dataframe(
                github_data[['repo_name', 'description', 'language', 'stars_today', 'total_stars', 'url']],
                column_config={
                    "url": st.column_config.LinkColumn("é“¾æ¥"),
                    "repo_name": "é¡¹ç›®åç§°",
                    "description": "ç®€ä»‹",
                    "language": "è¯­è¨€",
                    "stars_today": "ä»Šæ—¥ Star",
                    "total_stars": "æ€» Star"
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.info("æš‚æ—  GitHub æ•°æ®")

    with tab4:
        st.header("å°çº¢ä¹¦çƒ­ç‚¹ (ç¾å¦†/æ‹ç…§/å¥³ç”Ÿéœ€æ±‚)")
        st.caption("æ•°æ®æ¥æº: Bing Search (site:xiaohongshu.com)ï¼Œèšåˆå…³é”®è¯ï¼šç¾å¦†/æ‹ç…§/ç‹¬å±…/ç—›ç‚¹/éœ€æ±‚ (ä¸ä»…ä»…æ˜¯App)")
        if not xhs_data.empty:
            # ç¡®ä¿ date åˆ—å­˜åœ¨
            if 'date' not in xhs_data.columns:
                 xhs_data['date'] = selected_date.strftime('%Y-%m-%d')
            
            st.dataframe(
                xhs_data[['title', 'snippet', 'date', 'link']],
                column_config={
                    "link": st.column_config.LinkColumn("é“¾æ¥"),
                    "title": "æ ‡é¢˜",
                    "snippet": "å†…å®¹æ‘˜è¦",
                    "date": "æ—¥æœŸ"
                },
                hide_index=True,
                use_container_width=True
            )
        else:
            st.info("æš‚æ— å°çº¢ä¹¦æ•°æ®")
            # å¦‚æœæ²¡æœ‰é…ç½® Serper Keyï¼Œæ‰æ˜¾ç¤ºæç¤º
            if not st.secrets.get("SERPER_API_KEY") and not os.environ.get("SERPER_API_KEY"):
                st.warning("âš ï¸ **æŠ“å–æç¤º**ï¼šäº‘ç«¯ç¯å¢ƒ (Streamlit Cloud) å¯èƒ½ä¼šè¢« DuckDuckGo æ‹¦æˆªå¯¼è‡´æ— æ•°æ®ã€‚")
                st.markdown("""
                **å»ºè®®è§£å†³æ–¹æ¡ˆ (ä¸€åŠ³æ°¸é€¸)**ï¼š
                1. æ³¨å†Œ [Serper.dev](https://serper.dev/) (å…è´¹ï¼Œæ¯æœˆ2500æ¬¡è°ƒç”¨ï¼Œè¶³å¤Ÿä¸ªäººä½¿ç”¨)ã€‚
                2. è·å– API Keyã€‚
                3. åœ¨ Streamlit Secrets ä¸­æ·»åŠ  `SERPER_API_KEY = "ä½ çš„Key"`ã€‚
                4. åªæœ‰é…ç½®äº† Serper Keyï¼Œæ‰èƒ½ä¿è¯åœ¨äº‘ç«¯ç¨³å®šæŠ“å–æœç´¢ç»“æœã€‚
                """)
